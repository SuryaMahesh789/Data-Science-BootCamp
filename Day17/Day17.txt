Hadoop 1.x has several limitations when relying solely on MapReduce for data processing, which can impact performance, scalability, and usability. Here are some key limitations with examples:

 1. Limited to Batch Processing:
   - Limitation: Hadoop 1.x primarily supports batch processing through MapReduce, which is not suitable for real-time data processing or interactive querying.
   - Example: If you need to analyze streaming data (like live social media feeds), Hadoop 1.x cannot handle it efficiently, as it would require waiting for the entire batch to process before obtaining results.

 2. Single Job Tracker:
   - Limitation: Hadoop 1.x has a single JobTracker for managing jobs, which can become a bottleneck in large clusters, leading to reduced performance and reliability.





----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



Apache Spark is a powerful big data processing framework that supports various use cases due to its speed, scalability, and versatility. Here are some common use cases:

 1. Real-Time Data Processing:
   - Use Case: Processing real-time data streams from sources like social media feeds, IoT devices, or financial transactions.
   - Example: A financial institution uses Spark Streaming to detect fraudulent transactions in real-time, analyzing transaction patterns as they occur.

 2. Batch Processing:
   - Use Case: Large-scale data processing tasks that can be broken into smaller, parallelizable jobs.
   - Example: An e-commerce company uses Spark to process daily logs and generate reports on customer behavior, sales trends, and website performance.

 3. Machine Learning:
   - Use Case: Training and deploying machine learning models on large datasets.
   - Example: A healthcare provider uses Spark's MLlib to build predictive models for patient readmission rates, using vast amounts of patient data to train the models efficiently.

 4. Data Integration and ETL:
   - Use Case: Extracting, transforming, and loading (ETL) data from various sources into a unified data warehouse.
   - Example: A telecom company uses Spark to integrate data from different sources (like CRM, billing, and network systems), transform it into a common format, and load it into a centralized data store for analysis.

 5. Interactive Data Analysis:
   - Use Case: Performing exploratory data analysis (EDA) and interactive queries on large datasets.
   - Example: Data scientists use Spark with tools like Jupyter Notebook to interactively query and visualize large datasets, uncovering insights and trends.

 6. Graph Processing:
   - Use Case: Analyzing and processing large-scale graph data, such as social networks or recommendation systems.
   - Example: A social media platform uses Spark's GraphX to analyze user connections and recommend new friends or content based on their network structure.

 7. Handling Unstructured Data:
   - Use Case: Processing and analyzing unstructured or semi-structured data, such as text, images, or videos.
   - Example: A media company uses Spark to process large volumes of video files, extracting metadata and generating recommendations based on user viewing habits.

 8. Data Pipeline Orchestration:
   - Use Case: Building complex data pipelines that involve multiple stages of data processing and transformation.
   - Example: A logistics company uses Spark to automate the processing of sensor data from its fleet, transforming raw GPS data into actionable insights on delivery times and routes.

 9. Genomics and Bioinformatics:
   - Use Case: Analyzing massive datasets in genomics and bioinformatics.
   - Example: Researchers use Spark to process genome sequencing data, identifying patterns and mutations across large populations in a fraction of the time traditional methods would take.

Spark's ability to handle diverse workloads, from real-time processing to machine learning, makes it a versatile tool in many big data applications.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


YARN (Yet Another Resource Negotiator) and MapReduce are both integral parts of the Hadoop ecosystem, but they serve different purposes. Here's a comparison between the two:

 1. Purpose:
   - YARN: YARN is a resource management layer in Hadoop that allocates and manages resources (like CPU and memory) across all distributed applications running on a Hadoop cluster. It allows multiple data processing frameworks (including MapReduce, Spark, etc.) to run on the same cluster.
   - MapReduce: MapReduce is a programming model and processing engine for batch processing large datasets. It splits tasks into `Map` and `Reduce` phases, where the `Map` phase processes input data and the `Reduce` phase aggregates the results.

 2. Role in Hadoop:
   - YARN: Acts as the operating system for Hadoop by managing resources and scheduling jobs across the cluster. It decouples the resource management and job scheduling aspects from the data processing framework.
   - MapReduce: A data processing model and framework that runs on top of YARN (in Hadoop 2.x and later) to perform distributed data processing tasks.

 3. Flexibility:
   - YARN: Supports multiple processing frameworks beyond just MapReduce, such as Apache Spark, Apache Tez, and Apache Flink. This flexibility allows Hadoop to be used for a wider range of data processing tasks, including real-time processing, graph processing, and interactive querying.
   - MapReduce: Limited to batch processing tasks. It is designed for processing large volumes of data in a sequential manner and is not suitable for real-time processing or interactive querying.

 4. Scalability:
   - YARN: More scalable due to its ability to manage resources across multiple frameworks and applications simultaneously. It can handle more complex and diverse workloads compared to the traditional MapReduce framework.
   - MapReduce: Less scalable in the context of diverse workloads, as it was primarily designed for batch processing. The scalability of MapReduce is limited by its reliance on the JobTracker and TaskTracker in Hadoop 1.x.

 5. Resource Management:
   - YARN: Introduces a more efficient and flexible resource management system, where the ResourceManager manages resources and the ApplicationMaster manages job execution for each individual application.
   - MapReduce: In Hadoop 1.x, MapReduce itself handled resource management through a centralized JobTracker, which often became a bottleneck in large clusters.

 6. Performance:
   - YARN: Improves overall cluster utilization and performance by allowing multiple data processing engines to share cluster resources dynamically.
   - MapReduce: Performance is generally slower for certain workloads, especially those that are not well-suited to the MapReduce model, such as iterative algorithms used in machine learning or graph processing.

 Example:
- YARN: Suppose you have a Hadoop cluster running Spark, MapReduce, and HBase. YARN will manage the allocation of resources across these different frameworks, ensuring that they do not interfere with each other and that the cluster's resources are used efficiently.
- MapReduce: If you're processing large logs to count the number of occurrences of each word, MapReduce will split the task into multiple map tasks (each processing a part of the logs) and then reduce tasks to aggregate the results.

 Summary:
- YARN is the resource management layer that enhances Hadoop's flexibility, scalability, and performance by enabling multiple processing frameworks to run on the same cluster.
- MapReduce is a data processing model that runs on YARN (in Hadoop 2.x and later) and is specifically designed for batch processing large datasets. 

YARN's introduction in Hadoop 2.x was a significant evolution, as it allowed Hadoop to transcend beyond the limitations of MapReduce, supporting a broader range of data processing needs.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------







This function trains a logistic regression model with hyperparameter tuning and evaluates its accuracy:

1. Initialize Model: `LogisticRegression()` creates the logistic regression model.
2. Set Parameters: The `params` dictionary defines the hyperparameters to tune (`C` for regularization strength and `penalty` for L1/L2 regularization).
3. Grid Search: `GridSearchCV` performs cross-validated grid search over the specified hyperparameters.
4. Train Model: The model is trained on the training data using the best-found parameters.
5. Predict: Predictions are made on the test data.
6. Evaluate: Accuracy is calculated and printed.

The function outputs the accuracy of the logistic regression model on the test data.



def random_forest_learn(X_train, X_test, y_train, y_test):
    classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
    classifier.fit(X_train, y_train)
    y_pred2= classifier.predict(X_test)
    acc2= accuracy_score(y_test, y_pred2)
    print('Accuracy for RandomForest Classifier is: ', acc2)
    print('Accuracy for RandomForest Classifier in %: ', acc2100)



This function trains a Random Forest classifier and evaluates its accuracy:

1. Initialize Model: `RandomForestClassifier` is created with 10 trees (`n_estimators = 10`), using the 'entropy' criterion for splitting.
2. Train Model: The model is trained on the training data (`X_train`, `y_train`).
3. Predict: Predictions are made on the test data (`X_test`).
4. Evaluate: Accuracy is calculated using `accuracy_score` and printed.

The function outputs the accuracy of the Random Forest classifier on the test data.


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


This function trains a K-Nearest Neighbors (KNN) classifier with hyperparameter tuning and evaluates its accuracy:

1. Initialize Model: `KNeighborsClassifier()` creates the KNN model.
2. Set Parameters: `params1` defines the hyperparameters for tuning:
   - `n_neighbors`: Number of neighbors.
   - `leaf_size`: Size of the leaf in the tree.
   - `p`: Power parameter for the Minkowski metric.
   - `weights`: Weight function used in prediction.
   - `metric`: Distance metric used.
3. Grid Search: `GridSearchCV` performs a 5-fold cross-validated grid search over the hyperparameters.
4. Train Model: The model is trained on the training data with the best-found parameters.
5. Predict: Predictions are made on the test data.
6. Evaluate: Accuracy is calculated using `accuracy_score` and printed.

The function outputs the accuracy of the K-Nearest Neighbors classifier on the test data.


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------